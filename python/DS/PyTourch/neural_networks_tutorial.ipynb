{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "neural_networks_tutorial.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8dBznpP7MOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apBWNhHK7MOO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Neural Networks\n",
        "===============\n",
        "\n",
        "Neural networks can be constructed using the ``torch.nn`` package.\n",
        "\n",
        "Now that you had a glimpse of ``autograd``, ``nn`` depends on\n",
        "``autograd`` to define models and differentiate them.\n",
        "An ``nn.Module`` contains layers, and a method ``forward(input)``\\ that\n",
        "returns the ``output``.\n",
        "\n",
        "For example, look at this network that classifies digit images:\n",
        "\n",
        ".. figure:: /_static/img/mnist.png\n",
        "   :alt: convnet\n",
        "\n",
        "   convnet\n",
        "\n",
        "It is a simple feed-forward network. It takes the input, feeds it\n",
        "through several layers one after the other, and then finally gives the\n",
        "output.\n",
        "\n",
        "A typical training procedure for a neural network is as follows:\n",
        "\n",
        "- Define the neural network that has some learnable parameters (or\n",
        "  weights)\n",
        "- Iterate over a dataset of inputs\n",
        "- Process input through the network\n",
        "- Compute the loss (how far is the output from being correct)\n",
        "- Propagate gradients back into the network’s parameters\n",
        "- Update the weights of the network, typically using a simple update rule:\n",
        "  ``weight = weight - learning_rate * gradient``\n",
        "\n",
        "Define the network\n",
        "------------------\n",
        "\n",
        "Let’s define this network:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCPuQhac7MOP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5655276a-251f-4036-b1a3-cbfb10a75c54"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj1cwxYy7MOS",
        "colab_type": "text"
      },
      "source": [
        "You just have to define the ``forward`` function, and the ``backward``\n",
        "function (where gradients are computed) is automatically defined for you\n",
        "using ``autograd``.\n",
        "You can use any of the Tensor operations in the ``forward`` function.\n",
        "\n",
        "The learnable parameters of a model are returned by ``net.parameters()``\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxbKiKG_7MOT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0169bf8e-82ee-4d42-b898-2bb5186dbfb9"
      },
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size())  # conv1's .weight\n",
        "print(params)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "torch.Size([6, 1, 5, 5])\n",
            "[Parameter containing:\n",
            "tensor([[[[ 1.8006e-01, -6.5470e-03, -4.1899e-02,  6.2476e-02,  1.7562e-01],\n",
            "          [ 1.6524e-01, -3.4666e-02,  1.6265e-01, -7.3893e-03,  7.6667e-02],\n",
            "          [ 1.4140e-01, -4.7771e-02, -7.1506e-02, -2.4258e-02, -1.5113e-01],\n",
            "          [ 1.5610e-02,  6.9149e-02,  1.3380e-01,  1.5886e-01,  9.5024e-02],\n",
            "          [ 1.2258e-01, -6.5412e-02, -1.2615e-01,  1.8535e-01, -1.6216e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.7086e-01,  1.1331e-01, -1.9784e-01,  7.3296e-02, -1.5364e-01],\n",
            "          [ 1.2379e-01,  3.2287e-02, -5.2424e-02, -5.8609e-02, -1.8876e-01],\n",
            "          [ 5.9201e-02, -1.8658e-02, -3.2354e-02, -6.0745e-02,  4.6760e-02],\n",
            "          [-1.0457e-01,  1.6302e-01, -1.4674e-01,  1.0246e-01, -8.2287e-02],\n",
            "          [ 3.6631e-03, -3.3804e-02,  1.6463e-04,  8.9818e-02,  1.8786e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.8013e-02, -1.8568e-01, -7.2235e-02, -2.6775e-02,  1.0065e-01],\n",
            "          [-2.6225e-02, -1.8378e-01, -2.2473e-02, -7.0138e-03,  1.1627e-01],\n",
            "          [ 6.1857e-02, -6.1820e-02,  5.7983e-02,  2.5376e-02,  7.4113e-02],\n",
            "          [ 1.6439e-01, -1.3400e-01,  1.3976e-01,  1.1696e-01,  1.7269e-01],\n",
            "          [ 9.8427e-02, -1.5854e-01,  6.8280e-02,  1.4622e-01, -8.9461e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.7163e-01, -1.1932e-01, -1.4056e-01, -1.4966e-01, -1.5505e-01],\n",
            "          [-1.8926e-01, -1.1648e-01, -8.7286e-02,  9.7668e-02, -7.7956e-02],\n",
            "          [ 6.3350e-02,  6.9858e-02, -1.6651e-01,  1.6054e-02,  1.2901e-01],\n",
            "          [ 8.0034e-02,  1.7969e-01,  1.9748e-01, -4.1024e-02, -9.7270e-02],\n",
            "          [-8.4880e-02, -1.8812e-01,  1.1637e-01, -1.9734e-01, -1.6214e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1636e-01,  5.5982e-02,  1.9367e-01, -1.3885e-02, -2.7111e-02],\n",
            "          [-1.9486e-02, -1.3431e-01, -1.8856e-01,  1.7732e-01,  8.7778e-02],\n",
            "          [ 1.8564e-01,  4.4393e-02,  1.9428e-02,  1.3562e-01,  1.1914e-01],\n",
            "          [-4.1030e-02,  6.1037e-02, -9.8407e-02,  1.4132e-01, -1.2725e-01],\n",
            "          [ 8.3014e-02,  1.4244e-01, -2.5664e-02, -1.9592e-01,  1.6441e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 7.5991e-02, -1.2410e-01,  1.2022e-01,  1.4739e-01, -5.4269e-02],\n",
            "          [-6.2120e-02,  7.3863e-02,  1.7355e-01,  8.9382e-02,  1.1060e-01],\n",
            "          [-4.6811e-03, -1.9065e-01,  4.5351e-02, -5.1350e-02, -1.4025e-01],\n",
            "          [ 1.8071e-01,  1.4191e-01, -9.4718e-02,  3.7720e-02,  1.6516e-01],\n",
            "          [-7.2565e-02,  9.4707e-02,  1.0684e-01, -5.9626e-02,  1.1124e-02]]]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([-0.1854, -0.1661,  0.0984,  0.0512, -0.1599, -0.1340],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[[[-0.0627,  0.0652,  0.0078,  0.0038,  0.0257],\n",
            "          [ 0.0106,  0.0454, -0.0496,  0.0802, -0.0127],\n",
            "          [ 0.0651,  0.0149, -0.0725,  0.0131,  0.0015],\n",
            "          [-0.0481,  0.0313, -0.0448,  0.0340,  0.0397],\n",
            "          [ 0.0626,  0.0795, -0.0261,  0.0314, -0.0450]],\n",
            "\n",
            "         [[-0.0262, -0.0362, -0.0764,  0.0385, -0.0178],\n",
            "          [-0.0510, -0.0404,  0.0743,  0.0411, -0.0492],\n",
            "          [-0.0538, -0.0368,  0.0066,  0.0808, -0.0291],\n",
            "          [-0.0690,  0.0425, -0.0659, -0.0488, -0.0307],\n",
            "          [-0.0485, -0.0291,  0.0225, -0.0462,  0.0055]],\n",
            "\n",
            "         [[ 0.0538, -0.0093,  0.0343,  0.0060,  0.0226],\n",
            "          [ 0.0693, -0.0171, -0.0168, -0.0569, -0.0403],\n",
            "          [ 0.0570,  0.0343, -0.0436,  0.0554,  0.0803],\n",
            "          [ 0.0753,  0.0474,  0.0328, -0.0570,  0.0252],\n",
            "          [-0.0046,  0.0491, -0.0812, -0.0523,  0.0349]],\n",
            "\n",
            "         [[-0.0062,  0.0696, -0.0326,  0.0464, -0.0099],\n",
            "          [-0.0293,  0.0813,  0.0500,  0.0176,  0.0172],\n",
            "          [ 0.0676, -0.0613, -0.0581, -0.0664, -0.0394],\n",
            "          [-0.0703, -0.0693, -0.0059, -0.0070,  0.0251],\n",
            "          [-0.0334,  0.0600,  0.0208, -0.0049, -0.0074]],\n",
            "\n",
            "         [[ 0.0323,  0.0083, -0.0806,  0.0022,  0.0051],\n",
            "          [-0.0138,  0.0552,  0.0618, -0.0255,  0.0522],\n",
            "          [-0.0095, -0.0569,  0.0472, -0.0190,  0.0738],\n",
            "          [-0.0192, -0.0334, -0.0586,  0.0077,  0.0615],\n",
            "          [ 0.0768, -0.0224,  0.0573, -0.0792,  0.0109]],\n",
            "\n",
            "         [[ 0.0050, -0.0810, -0.0152,  0.0585, -0.0658],\n",
            "          [ 0.0512, -0.0013,  0.0294,  0.0610, -0.0277],\n",
            "          [-0.0771, -0.0286,  0.0755, -0.0145,  0.0539],\n",
            "          [ 0.0440, -0.0624,  0.0705,  0.0710,  0.0009],\n",
            "          [-0.0353,  0.0344,  0.0682, -0.0278, -0.0490]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0338,  0.0345, -0.0350, -0.0049,  0.0624],\n",
            "          [-0.0711, -0.0152, -0.0344, -0.0700,  0.0144],\n",
            "          [-0.0031, -0.0793, -0.0341,  0.0490, -0.0615],\n",
            "          [ 0.0673, -0.0679,  0.0199,  0.0411, -0.0066],\n",
            "          [-0.0448, -0.0589,  0.0479,  0.0036,  0.0759]],\n",
            "\n",
            "         [[ 0.0055,  0.0711,  0.0285,  0.0779,  0.0701],\n",
            "          [-0.0728, -0.0776, -0.0150,  0.0037, -0.0100],\n",
            "          [ 0.0139,  0.0521, -0.0276,  0.0185, -0.0308],\n",
            "          [ 0.0724,  0.0392,  0.0768,  0.0524, -0.0447],\n",
            "          [-0.0404,  0.0197, -0.0301,  0.0767, -0.0225]],\n",
            "\n",
            "         [[ 0.0326,  0.0201, -0.0750,  0.0681,  0.0608],\n",
            "          [ 0.0408,  0.0695,  0.0043,  0.0596,  0.0148],\n",
            "          [-0.0668, -0.0572,  0.0369, -0.0306,  0.0731],\n",
            "          [ 0.0506, -0.0552, -0.0218,  0.0139,  0.0793],\n",
            "          [-0.0288,  0.0113,  0.0743, -0.0048,  0.0749]],\n",
            "\n",
            "         [[-0.0135,  0.0482,  0.0212, -0.0450,  0.0673],\n",
            "          [-0.0497, -0.0648,  0.0601, -0.0427,  0.0225],\n",
            "          [ 0.0271, -0.0326, -0.0344,  0.0323,  0.0332],\n",
            "          [-0.0370,  0.0140, -0.0028,  0.0750,  0.0225],\n",
            "          [ 0.0233, -0.0343,  0.0580,  0.0342, -0.0419]],\n",
            "\n",
            "         [[ 0.0244,  0.0377,  0.0189, -0.0793, -0.0064],\n",
            "          [ 0.0790,  0.0406, -0.0192, -0.0374, -0.0148],\n",
            "          [ 0.0793, -0.0312,  0.0138,  0.0013, -0.0612],\n",
            "          [ 0.0443, -0.0807,  0.0217, -0.0220,  0.0507],\n",
            "          [ 0.0359,  0.0158, -0.0538,  0.0279,  0.0064]],\n",
            "\n",
            "         [[-0.0299, -0.0271, -0.0035,  0.0345, -0.0704],\n",
            "          [ 0.0213,  0.0285, -0.0070, -0.0182, -0.0258],\n",
            "          [-0.0522, -0.0084,  0.0465,  0.0509, -0.0452],\n",
            "          [-0.0234,  0.0731, -0.0759,  0.0246, -0.0035],\n",
            "          [ 0.0508,  0.0283,  0.0236,  0.0287,  0.0798]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0494, -0.0558,  0.0365,  0.0765,  0.0111],\n",
            "          [ 0.0321,  0.0021, -0.0514, -0.0100, -0.0118],\n",
            "          [ 0.0682, -0.0237, -0.0645, -0.0327,  0.0546],\n",
            "          [ 0.0040,  0.0272, -0.0702,  0.0704, -0.0135],\n",
            "          [-0.0147, -0.0299, -0.0114,  0.0134, -0.0415]],\n",
            "\n",
            "         [[-0.0361, -0.0419, -0.0293, -0.0547, -0.0484],\n",
            "          [ 0.0404,  0.0657, -0.0053, -0.0040, -0.0603],\n",
            "          [-0.0780, -0.0796, -0.0693, -0.0303, -0.0589],\n",
            "          [ 0.0225,  0.0564,  0.0119, -0.0452, -0.0363],\n",
            "          [-0.0250, -0.0357, -0.0571,  0.0281,  0.0639]],\n",
            "\n",
            "         [[ 0.0060,  0.0236, -0.0757,  0.0228,  0.0615],\n",
            "          [ 0.0492, -0.0368, -0.0208, -0.0102,  0.0499],\n",
            "          [ 0.0576,  0.0061,  0.0122, -0.0134, -0.0724],\n",
            "          [ 0.0444, -0.0517,  0.0426, -0.0125,  0.0264],\n",
            "          [-0.0108,  0.0304, -0.0538, -0.0176, -0.0314]],\n",
            "\n",
            "         [[ 0.0353, -0.0505,  0.0486,  0.0170,  0.0219],\n",
            "          [ 0.0347,  0.0276,  0.0597,  0.0179, -0.0607],\n",
            "          [ 0.0168,  0.0302, -0.0171, -0.0008,  0.0274],\n",
            "          [-0.0813, -0.0503,  0.0615, -0.0505, -0.0754],\n",
            "          [-0.0787,  0.0579,  0.0808, -0.0334, -0.0708]],\n",
            "\n",
            "         [[-0.0043, -0.0356, -0.0107,  0.0661,  0.0139],\n",
            "          [-0.0702, -0.0211,  0.0794,  0.0738,  0.0141],\n",
            "          [-0.0500, -0.0265,  0.0559, -0.0007, -0.0681],\n",
            "          [-0.0670,  0.0490,  0.0769, -0.0056, -0.0272],\n",
            "          [-0.0801,  0.0670,  0.0614, -0.0229,  0.0044]],\n",
            "\n",
            "         [[ 0.0569, -0.0528,  0.0602,  0.0620, -0.0461],\n",
            "          [-0.0605, -0.0207, -0.0277,  0.0415,  0.0452],\n",
            "          [ 0.0707, -0.0341,  0.0241,  0.0152, -0.0482],\n",
            "          [ 0.0444, -0.0456,  0.0297, -0.0741,  0.0525],\n",
            "          [-0.0776, -0.0139, -0.0474,  0.0690, -0.0591]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0135,  0.0509,  0.0227, -0.0201, -0.0434],\n",
            "          [-0.0154, -0.0329, -0.0377,  0.0356, -0.0288],\n",
            "          [ 0.0709,  0.0583, -0.0181,  0.0111,  0.0407],\n",
            "          [ 0.0317,  0.0452,  0.0508, -0.0795, -0.0745],\n",
            "          [ 0.0493, -0.0300,  0.0578,  0.0409,  0.0410]],\n",
            "\n",
            "         [[-0.0523,  0.0207,  0.0203,  0.0337,  0.0656],\n",
            "          [ 0.0306,  0.0438,  0.0438, -0.0555,  0.0111],\n",
            "          [-0.0586, -0.0152, -0.0609, -0.0038, -0.0741],\n",
            "          [ 0.0080,  0.0458, -0.0624, -0.0139, -0.0609],\n",
            "          [ 0.0485,  0.0572, -0.0370,  0.0156,  0.0336]],\n",
            "\n",
            "         [[-0.0633, -0.0521, -0.0664,  0.0305, -0.0184],\n",
            "          [-0.0407,  0.0377,  0.0639, -0.0113, -0.0221],\n",
            "          [ 0.0495, -0.0277,  0.0676, -0.0756,  0.0720],\n",
            "          [ 0.0257, -0.0800, -0.0408, -0.0736, -0.0494],\n",
            "          [ 0.0210,  0.0348, -0.0311,  0.0183, -0.0428]],\n",
            "\n",
            "         [[ 0.0012,  0.0237, -0.0202, -0.0039,  0.0359],\n",
            "          [ 0.0528,  0.0373, -0.0260,  0.0005, -0.0781],\n",
            "          [-0.0351,  0.0632,  0.0654, -0.0306, -0.0574],\n",
            "          [-0.0429, -0.0651, -0.0654,  0.0162, -0.0420],\n",
            "          [ 0.0794, -0.0276, -0.0636, -0.0676, -0.0481]],\n",
            "\n",
            "         [[ 0.0277, -0.0287, -0.0771, -0.0631, -0.0780],\n",
            "          [-0.0215,  0.0591,  0.0421,  0.0737,  0.0531],\n",
            "          [ 0.0313, -0.0316, -0.0304,  0.0793,  0.0605],\n",
            "          [ 0.0240, -0.0815,  0.0158,  0.0182,  0.0143],\n",
            "          [-0.0197,  0.0166, -0.0143, -0.0029,  0.0064]],\n",
            "\n",
            "         [[ 0.0151, -0.0003,  0.0672,  0.0247, -0.0656],\n",
            "          [ 0.0695, -0.0081, -0.0208, -0.0609,  0.0406],\n",
            "          [-0.0652,  0.0804,  0.0460, -0.0472, -0.0200],\n",
            "          [ 0.0468, -0.0439,  0.0309,  0.0726, -0.0488],\n",
            "          [ 0.0511, -0.0622, -0.0149, -0.0061,  0.0759]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0607,  0.0095,  0.0079, -0.0182,  0.0645],\n",
            "          [-0.0749, -0.0150, -0.0007,  0.0742,  0.0356],\n",
            "          [-0.0036, -0.0271,  0.0414,  0.0576, -0.0256],\n",
            "          [ 0.0168,  0.0233,  0.0059,  0.0592, -0.0110],\n",
            "          [-0.0646, -0.0013, -0.0655,  0.0722, -0.0073]],\n",
            "\n",
            "         [[-0.0150,  0.0221, -0.0344, -0.0128,  0.0808],\n",
            "          [-0.0502,  0.0158,  0.0455, -0.0320,  0.0050],\n",
            "          [ 0.0515, -0.0366, -0.0387, -0.0594, -0.0292],\n",
            "          [ 0.0366,  0.0560,  0.0021, -0.0468, -0.0068],\n",
            "          [ 0.0010, -0.0058, -0.0069,  0.0124, -0.0065]],\n",
            "\n",
            "         [[ 0.0590,  0.0139,  0.0294,  0.0013,  0.0675],\n",
            "          [ 0.0811, -0.0457,  0.0659, -0.0054,  0.0167],\n",
            "          [ 0.0306,  0.0611, -0.0545,  0.0622, -0.0356],\n",
            "          [ 0.0659,  0.0243,  0.0658, -0.0795,  0.0585],\n",
            "          [-0.0303, -0.0539,  0.0269,  0.0373, -0.0670]],\n",
            "\n",
            "         [[-0.0664,  0.0398,  0.0056,  0.0740, -0.0026],\n",
            "          [-0.0190,  0.0475, -0.0640,  0.0078, -0.0590],\n",
            "          [ 0.0575,  0.0131, -0.0503, -0.0307,  0.0773],\n",
            "          [-0.0266,  0.0048, -0.0506, -0.0744, -0.0539],\n",
            "          [ 0.0346, -0.0636,  0.0379,  0.0619, -0.0545]],\n",
            "\n",
            "         [[-0.0075, -0.0182, -0.0773, -0.0296, -0.0245],\n",
            "          [ 0.0471, -0.0019, -0.0581,  0.0758, -0.0231],\n",
            "          [-0.0778, -0.0724, -0.0470, -0.0791,  0.0308],\n",
            "          [ 0.0380,  0.0759, -0.0026, -0.0698, -0.0339],\n",
            "          [-0.0658,  0.0200,  0.0766, -0.0383, -0.0768]],\n",
            "\n",
            "         [[-0.0426,  0.0771, -0.0093,  0.0698,  0.0771],\n",
            "          [-0.0714, -0.0569, -0.0694,  0.0486, -0.0567],\n",
            "          [-0.0255, -0.0065, -0.0063,  0.0396,  0.0595],\n",
            "          [-0.0122,  0.0441,  0.0147, -0.0264, -0.0682],\n",
            "          [ 0.0444,  0.0284,  0.0009, -0.0668,  0.0410]]],\n",
            "\n",
            "\n",
            "        [[[-0.0362, -0.0536, -0.0547, -0.0042, -0.0535],\n",
            "          [ 0.0795, -0.0453,  0.0010,  0.0778, -0.0424],\n",
            "          [-0.0032,  0.0070,  0.0620,  0.0319, -0.0688],\n",
            "          [ 0.0704,  0.0232, -0.0590, -0.0621,  0.0031],\n",
            "          [-0.0504, -0.0083, -0.0133,  0.0333,  0.0431]],\n",
            "\n",
            "         [[ 0.0275, -0.0684, -0.0768,  0.0639,  0.0632],\n",
            "          [ 0.0684, -0.0649, -0.0488, -0.0510, -0.0378],\n",
            "          [-0.0365,  0.0565, -0.0763, -0.0234,  0.0385],\n",
            "          [ 0.0650,  0.0147,  0.0415, -0.0330,  0.0127],\n",
            "          [ 0.0443, -0.0340,  0.0067, -0.0314, -0.0304]],\n",
            "\n",
            "         [[ 0.0787,  0.0327,  0.0462, -0.0057, -0.0520],\n",
            "          [ 0.0642, -0.0561,  0.0762, -0.0107, -0.0609],\n",
            "          [ 0.0247, -0.0165, -0.0333, -0.0516, -0.0562],\n",
            "          [-0.0111,  0.0062, -0.0278, -0.0770, -0.0458],\n",
            "          [ 0.0807,  0.0704,  0.0611,  0.0441, -0.0123]],\n",
            "\n",
            "         [[-0.0312, -0.0002,  0.0320,  0.0111,  0.0671],\n",
            "          [ 0.0700, -0.0234, -0.0083, -0.0477, -0.0517],\n",
            "          [ 0.0334, -0.0129,  0.0195,  0.0682,  0.0658],\n",
            "          [-0.0738, -0.0144, -0.0103, -0.0188,  0.0760],\n",
            "          [-0.0553, -0.0161,  0.0149, -0.0031,  0.0394]],\n",
            "\n",
            "         [[ 0.0486, -0.0244, -0.0683,  0.0119, -0.0756],\n",
            "          [-0.0717, -0.0177, -0.0384, -0.0189,  0.0276],\n",
            "          [ 0.0353,  0.0654,  0.0715, -0.0195,  0.0760],\n",
            "          [ 0.0271, -0.0777, -0.0087,  0.0224, -0.0362],\n",
            "          [ 0.0343, -0.0154, -0.0713, -0.0490, -0.0597]],\n",
            "\n",
            "         [[ 0.0319, -0.0569, -0.0114,  0.0626, -0.0654],\n",
            "          [ 0.0811,  0.0577, -0.0672,  0.0482, -0.0688],\n",
            "          [-0.0270,  0.0807,  0.0262,  0.0397,  0.0076],\n",
            "          [-0.0410,  0.0790,  0.0578, -0.0276,  0.0011],\n",
            "          [ 0.0474,  0.0548, -0.0032, -0.0554, -0.0404]]]], requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0393, -0.0387,  0.0489, -0.0044, -0.0369, -0.0245,  0.0350,  0.0463,\n",
            "        -0.0085, -0.0506, -0.0749,  0.0090,  0.0465, -0.0381,  0.0446, -0.0552],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.0089, -0.0411,  0.0119,  ..., -0.0268, -0.0014, -0.0138],\n",
            "        [ 0.0085, -0.0175,  0.0182,  ..., -0.0168, -0.0366, -0.0440],\n",
            "        [-0.0243, -0.0117, -0.0258,  ...,  0.0274, -0.0306,  0.0302],\n",
            "        ...,\n",
            "        [ 0.0223, -0.0379, -0.0256,  ..., -0.0065, -0.0126,  0.0439],\n",
            "        [-0.0203,  0.0335,  0.0390,  ...,  0.0061,  0.0168,  0.0059],\n",
            "        [-0.0203, -0.0492,  0.0293,  ..., -0.0476,  0.0310, -0.0466]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([-0.0416, -0.0342, -0.0453, -0.0382, -0.0119,  0.0164,  0.0485,  0.0470,\n",
            "        -0.0133,  0.0303,  0.0141,  0.0101, -0.0496,  0.0170,  0.0130,  0.0069,\n",
            "         0.0022, -0.0337,  0.0419,  0.0335,  0.0376,  0.0459,  0.0070, -0.0485,\n",
            "        -0.0021, -0.0207, -0.0017,  0.0020, -0.0169,  0.0421,  0.0318,  0.0402,\n",
            "         0.0380, -0.0449, -0.0106,  0.0052,  0.0039,  0.0010, -0.0212, -0.0421,\n",
            "         0.0286, -0.0392, -0.0235, -0.0136, -0.0111,  0.0448,  0.0085,  0.0323,\n",
            "         0.0138, -0.0428, -0.0171, -0.0345, -0.0025, -0.0491, -0.0346,  0.0316,\n",
            "         0.0230, -0.0350, -0.0457, -0.0447, -0.0294,  0.0305,  0.0196, -0.0258,\n",
            "         0.0437, -0.0441, -0.0489, -0.0194,  0.0319,  0.0215, -0.0036, -0.0440,\n",
            "        -0.0448,  0.0458,  0.0382, -0.0335,  0.0091, -0.0486,  0.0337, -0.0312,\n",
            "        -0.0403, -0.0172, -0.0175,  0.0073, -0.0289, -0.0203,  0.0317,  0.0071,\n",
            "         0.0184,  0.0016, -0.0465, -0.0072, -0.0081, -0.0175,  0.0471, -0.0480,\n",
            "         0.0015,  0.0019, -0.0279, -0.0482, -0.0304, -0.0400,  0.0074, -0.0325,\n",
            "        -0.0285,  0.0471,  0.0400,  0.0442,  0.0422,  0.0121, -0.0301,  0.0231,\n",
            "         0.0442, -0.0340, -0.0301,  0.0023,  0.0300, -0.0136, -0.0199, -0.0257],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[-0.0084, -0.0788, -0.0243,  ..., -0.0542,  0.0913, -0.0906],\n",
            "        [ 0.0456,  0.0386, -0.0879,  ..., -0.0904, -0.0413, -0.0137],\n",
            "        [ 0.0548, -0.0773, -0.0259,  ..., -0.0197, -0.0142, -0.0428],\n",
            "        ...,\n",
            "        [ 0.0560, -0.0896,  0.0202,  ..., -0.0623,  0.0840,  0.0333],\n",
            "        [ 0.0709,  0.0046, -0.0468,  ...,  0.0503, -0.0504,  0.0353],\n",
            "        [ 0.0280,  0.0487,  0.0112,  ...,  0.0856, -0.0137,  0.0368]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([-0.0118,  0.0145,  0.0594, -0.0605, -0.0702, -0.0607,  0.0671, -0.0422,\n",
            "        -0.0803,  0.0378, -0.0404,  0.0649, -0.0171,  0.0175,  0.0629, -0.0744,\n",
            "        -0.0700, -0.0774, -0.0024, -0.0819, -0.0079, -0.0575, -0.0806,  0.0269,\n",
            "         0.0581,  0.0348, -0.0052,  0.0280, -0.0665,  0.0618,  0.0767, -0.0386,\n",
            "        -0.0791, -0.0606, -0.0528, -0.0418, -0.0568, -0.0868,  0.0213,  0.0545,\n",
            "         0.0349, -0.0159,  0.0739, -0.0320,  0.0763,  0.0526,  0.0421,  0.0122,\n",
            "        -0.0777, -0.0378,  0.0222, -0.0579, -0.0048, -0.0849, -0.0598, -0.0098,\n",
            "         0.0372,  0.0117, -0.0444, -0.0453, -0.0848,  0.0269,  0.0467, -0.0846,\n",
            "        -0.0906, -0.0745,  0.0369,  0.0039,  0.0535,  0.0118,  0.0025, -0.0214,\n",
            "         0.0359, -0.0256,  0.0903,  0.0656, -0.0099,  0.0772, -0.0014, -0.0749,\n",
            "         0.0296, -0.0127,  0.0344, -0.0787], requires_grad=True), Parameter containing:\n",
            "tensor([[ 6.1639e-02, -8.5977e-02, -1.0452e-01, -2.7542e-02, -3.2428e-02,\n",
            "         -3.1046e-02,  2.4269e-02, -6.8129e-02,  7.3498e-02,  8.2446e-02,\n",
            "         -2.7433e-02,  3.4910e-02, -7.0452e-02, -3.2518e-02,  4.1089e-02,\n",
            "         -9.5526e-02,  5.5644e-02, -4.9282e-02, -9.2915e-02,  1.2852e-02,\n",
            "          7.1972e-02, -1.0538e-01, -1.0856e-01,  3.1798e-02, -1.0477e-01,\n",
            "          9.2641e-02,  5.9541e-02, -4.6420e-02, -6.4441e-02, -7.6901e-02,\n",
            "          2.1058e-02,  4.4747e-02,  4.5834e-02, -2.1414e-02, -4.3118e-03,\n",
            "         -1.5722e-02, -8.9376e-02,  4.6272e-02, -1.4357e-02,  4.7307e-02,\n",
            "         -9.1065e-02,  7.9253e-02, -1.4494e-02,  9.4936e-02, -4.2171e-02,\n",
            "         -8.1540e-02, -2.0339e-02, -6.1429e-02,  4.4402e-02, -2.3497e-02,\n",
            "          1.0787e-01,  1.8956e-02,  3.9400e-02, -7.2835e-02,  8.6934e-02,\n",
            "          2.2021e-02,  5.9528e-02, -1.9497e-02, -2.8864e-03,  5.8218e-02,\n",
            "          2.6003e-03, -8.9187e-02, -1.3421e-02, -7.5963e-02, -9.0033e-02,\n",
            "         -7.1492e-02,  2.1533e-02,  4.6548e-02, -6.4650e-02,  7.3226e-02,\n",
            "          1.9620e-03,  3.8713e-02, -5.4599e-03, -2.2893e-03,  1.0723e-01,\n",
            "          6.6867e-02, -1.6834e-03, -9.4692e-02,  2.2364e-02, -2.6924e-02,\n",
            "          2.0977e-02, -1.0636e-01, -3.9477e-02, -2.6938e-02],\n",
            "        [ 6.2911e-02, -6.8142e-02, -6.0162e-02, -1.0457e-01, -1.0829e-01,\n",
            "          5.1647e-02,  6.5097e-02,  3.9522e-03, -9.0355e-02,  9.8761e-02,\n",
            "          9.6741e-02, -6.0355e-02,  1.9967e-02, -2.1640e-02,  6.2368e-02,\n",
            "          4.1704e-02,  1.4815e-02,  9.1872e-03, -5.4328e-02,  8.4230e-03,\n",
            "          7.9612e-02,  1.0855e-01,  6.7473e-02, -4.7247e-02,  1.0840e-01,\n",
            "          9.9498e-02,  3.5140e-02, -6.5536e-02, -3.5314e-02,  2.9615e-02,\n",
            "          7.1037e-02, -6.1762e-02,  8.8337e-02,  3.0952e-02,  3.0103e-02,\n",
            "          3.0821e-02, -8.2161e-03, -4.5795e-02,  5.6859e-02,  7.7671e-02,\n",
            "         -8.5560e-02,  9.0726e-02,  5.6527e-02,  3.6413e-02, -1.0343e-01,\n",
            "         -3.3907e-02, -1.1857e-02,  4.9491e-02,  1.9082e-02, -6.7094e-02,\n",
            "          1.0161e-01,  6.2852e-02, -2.9645e-02,  2.2751e-02,  8.1030e-02,\n",
            "          6.8032e-02,  4.9802e-02,  1.0846e-01,  6.5903e-02, -2.4343e-02,\n",
            "         -2.6795e-02, -1.6448e-02,  2.8252e-02,  7.3380e-02, -2.8206e-02,\n",
            "         -9.2428e-02, -3.1158e-02,  4.1191e-02,  1.3759e-02,  1.6551e-02,\n",
            "          8.5205e-03, -1.7532e-02, -2.7462e-02, -2.1878e-02,  9.3920e-02,\n",
            "          8.4326e-02, -3.0051e-02, -6.9335e-02,  1.0292e-01, -2.3324e-02,\n",
            "         -7.8164e-02,  4.2914e-02, -8.6326e-02,  1.0738e-01],\n",
            "        [-4.5672e-02, -2.9339e-02, -5.2265e-03,  9.3397e-03,  4.2090e-02,\n",
            "         -1.4999e-02, -7.9214e-02, -3.4053e-03, -7.1998e-02, -3.7806e-02,\n",
            "         -4.4285e-02, -5.1118e-02,  5.4978e-02, -4.1033e-02,  7.1112e-02,\n",
            "         -8.2643e-02, -3.7498e-02, -6.0833e-02, -7.3441e-02, -1.0376e-01,\n",
            "         -1.0781e-01,  4.0239e-02, -4.7077e-02,  3.0084e-03, -2.6847e-02,\n",
            "         -4.4299e-02,  2.3818e-02, -1.8865e-02, -1.0616e-01, -1.0461e-01,\n",
            "         -7.6391e-02, -2.5756e-02, -5.4589e-02, -5.5273e-02, -3.8972e-02,\n",
            "          9.0805e-02,  9.9961e-02,  1.0565e-05,  5.8809e-02,  2.0641e-02,\n",
            "          4.0845e-02, -4.7629e-02, -1.5657e-03,  9.0103e-02, -5.1763e-02,\n",
            "          4.4054e-02, -4.4906e-02,  1.0462e-02,  4.2299e-02, -1.0138e-01,\n",
            "          3.8366e-02, -3.7828e-02, -3.1150e-02,  7.6316e-02, -6.8763e-02,\n",
            "         -3.6163e-02, -5.3278e-02, -8.0546e-02, -7.3000e-02, -3.8133e-02,\n",
            "         -5.8909e-02,  6.0498e-02,  5.7008e-02,  3.3393e-02, -8.1503e-02,\n",
            "         -1.0643e-01,  2.3626e-02,  1.0463e-01,  6.0881e-02, -8.1807e-03,\n",
            "         -1.3166e-02,  6.9336e-02,  9.9537e-02, -4.8090e-02,  8.4625e-03,\n",
            "          2.6875e-02,  5.8401e-02, -1.2484e-02,  5.9964e-02,  1.8459e-02,\n",
            "         -4.4457e-02, -4.8712e-02,  2.1582e-02, -9.5144e-02],\n",
            "        [ 4.7330e-03,  5.1913e-02, -6.0916e-02,  1.0257e-03, -1.3865e-02,\n",
            "         -8.5590e-02,  4.5165e-02, -8.1424e-02,  7.1059e-02, -7.5531e-02,\n",
            "          1.0265e-01,  7.3237e-02, -7.8618e-02,  1.9050e-02,  2.3455e-02,\n",
            "          9.9734e-02, -2.8688e-02,  7.7447e-02, -4.8570e-05,  7.9729e-02,\n",
            "         -1.0177e-01,  5.7077e-02, -2.5798e-02, -4.2025e-03,  3.8369e-03,\n",
            "         -9.1981e-02,  3.6523e-02, -3.0575e-02, -3.8788e-02,  3.7407e-02,\n",
            "          6.2396e-02,  6.0208e-02,  3.7685e-02,  9.1204e-02,  5.5584e-02,\n",
            "         -8.2859e-02,  7.5863e-02, -1.0704e-01, -7.1295e-02, -5.7752e-02,\n",
            "         -8.6838e-02,  1.5733e-02,  4.3007e-02, -8.6106e-02,  5.6537e-02,\n",
            "         -5.9555e-02, -1.3880e-02,  3.5235e-02,  7.4722e-02, -5.7820e-02,\n",
            "         -7.3668e-02,  9.2038e-02, -1.0334e-01,  6.6421e-02,  7.6821e-02,\n",
            "          3.4755e-02, -3.8001e-02, -5.8245e-02, -5.8459e-02,  1.3422e-02,\n",
            "          8.9322e-02, -8.4304e-02, -5.4635e-02,  5.6231e-02, -9.1359e-02,\n",
            "          5.1453e-02, -7.6100e-02, -8.8993e-02,  6.8191e-02,  9.0932e-02,\n",
            "          1.0217e-01, -6.6694e-02,  6.3633e-02,  9.1461e-02,  3.2970e-02,\n",
            "         -9.7857e-03, -3.9567e-02,  6.2883e-02, -1.0335e-01,  1.0393e-02,\n",
            "          8.1206e-02,  2.2812e-02,  8.6246e-02, -4.0542e-02],\n",
            "        [-1.0469e-01,  5.1562e-04, -1.0716e-01,  8.6291e-02, -1.5363e-02,\n",
            "          5.1838e-02, -5.0940e-02,  1.0819e-01,  4.2527e-02,  1.8424e-02,\n",
            "         -6.1010e-02, -3.6260e-02,  3.6759e-02,  1.0258e-01, -7.0923e-02,\n",
            "         -3.1797e-02,  9.8489e-02,  5.1153e-02, -4.1819e-02, -5.5551e-02,\n",
            "         -3.2955e-02, -1.6931e-02, -1.0608e-01,  6.5158e-02, -6.0082e-02,\n",
            "         -6.8615e-02,  1.0764e-01, -8.1386e-02, -1.4839e-04, -3.1637e-02,\n",
            "         -5.5419e-02, -4.0312e-02, -9.0523e-02,  7.1365e-02, -2.6549e-02,\n",
            "          8.7013e-02,  9.1847e-03, -6.4460e-02,  2.5276e-02, -4.7600e-02,\n",
            "         -7.6975e-02,  1.5600e-02,  6.9300e-02, -8.9988e-02,  5.7896e-02,\n",
            "          9.9844e-02, -9.9875e-02,  9.6813e-02,  5.8474e-02,  2.6984e-02,\n",
            "         -8.1812e-02, -7.8346e-02, -7.5296e-02, -3.5933e-02, -1.7298e-02,\n",
            "         -6.0791e-02,  7.8324e-02, -9.8638e-02, -5.2160e-02,  1.0298e-01,\n",
            "         -1.2244e-02, -2.1877e-02,  7.4539e-02,  8.7550e-02, -7.9031e-02,\n",
            "          4.5172e-03,  3.2735e-02,  8.8285e-02,  5.5756e-02,  2.2571e-02,\n",
            "         -2.9983e-02, -4.9011e-02, -6.2233e-02,  8.9962e-02,  8.3298e-02,\n",
            "         -9.0007e-03, -6.7201e-02,  1.7372e-02, -1.0235e-01,  1.0536e-01,\n",
            "         -5.7156e-02, -7.8017e-02,  7.9933e-02, -6.6418e-02],\n",
            "        [ 8.4602e-02,  9.5654e-02, -9.1102e-02, -3.6149e-02, -7.9437e-03,\n",
            "          3.6221e-02, -1.8620e-02,  5.5115e-02,  3.1974e-02, -9.6691e-03,\n",
            "          7.0269e-02, -6.9239e-02,  5.8185e-03, -7.9345e-02, -1.5797e-02,\n",
            "          7.6594e-02,  7.6982e-02,  2.4944e-02, -5.3761e-02,  9.0888e-02,\n",
            "          9.0016e-02, -9.9048e-02,  8.5829e-02,  9.5007e-02, -9.8618e-02,\n",
            "          4.8263e-02,  3.6719e-02,  4.8719e-02,  4.3006e-02,  9.7017e-02,\n",
            "          6.5333e-02, -5.3190e-02, -9.0711e-02, -1.4278e-02,  1.3568e-02,\n",
            "         -3.1576e-02, -1.0908e-01, -6.5400e-03, -5.8723e-02, -4.7705e-02,\n",
            "          1.0553e-01,  8.4116e-02, -1.0405e-01,  7.1218e-02, -9.7203e-03,\n",
            "         -3.7301e-02, -1.8662e-02,  8.8174e-02, -6.5251e-02, -6.4389e-02,\n",
            "         -5.2066e-03,  9.9610e-02, -8.1401e-02, -8.5746e-02, -9.4680e-02,\n",
            "          7.1596e-02,  2.5368e-02, -5.6773e-02,  9.5239e-02,  8.1334e-03,\n",
            "          2.9603e-02, -3.1406e-02,  4.9671e-03, -4.5587e-02, -8.4893e-02,\n",
            "          4.7670e-02, -1.8992e-02, -1.0364e-01, -7.0768e-02,  4.5805e-02,\n",
            "         -2.4136e-02,  3.4171e-02, -8.7964e-02,  3.2432e-02, -5.3852e-02,\n",
            "          6.4520e-02,  8.8405e-02, -1.3487e-02,  4.3512e-02,  8.6278e-02,\n",
            "          4.1452e-03, -4.6356e-04,  1.0887e-01, -7.2553e-02],\n",
            "        [ 1.0312e-01,  5.3657e-02,  9.0342e-02, -7.0323e-02, -9.9858e-02,\n",
            "          5.5233e-02,  6.3937e-02, -2.0841e-03,  5.0412e-02, -6.7795e-02,\n",
            "          5.7092e-02,  6.0828e-02,  4.1117e-02, -4.4460e-02,  7.8404e-02,\n",
            "         -1.1607e-02, -1.0096e-01,  6.0813e-02,  8.5097e-02, -7.9598e-03,\n",
            "         -4.4621e-02,  9.9341e-02, -4.2768e-02,  2.4237e-02,  9.8097e-02,\n",
            "         -4.1524e-02,  4.2864e-02, -3.3228e-02, -4.8382e-02, -5.7179e-03,\n",
            "         -5.4731e-02, -3.2738e-02,  3.2172e-02, -1.0620e-01,  4.2958e-02,\n",
            "         -5.9688e-02, -5.9056e-02,  6.6446e-02,  9.3459e-02,  9.1105e-02,\n",
            "         -5.8914e-02,  4.6222e-03, -6.3214e-02, -3.7700e-02,  2.3784e-02,\n",
            "         -8.7766e-02, -8.4751e-02,  6.8760e-02,  8.4033e-02, -5.5583e-02,\n",
            "         -1.0260e-01,  2.0264e-02, -8.3741e-02, -8.9744e-02, -5.5100e-02,\n",
            "          2.2199e-02,  1.0800e-01, -6.6310e-02,  5.8655e-02, -7.7599e-03,\n",
            "         -6.7286e-02,  6.6219e-02, -5.5065e-02, -5.4955e-02, -9.7943e-02,\n",
            "         -1.0739e-01,  7.2164e-03, -1.0096e-01,  5.6903e-02, -2.9798e-02,\n",
            "          8.4751e-02,  1.0669e-01, -3.5709e-03, -6.7662e-02, -2.2716e-02,\n",
            "          3.8792e-02, -1.6331e-02, -8.9945e-02, -1.0867e-01,  9.1062e-03,\n",
            "          8.1961e-02,  5.3235e-02, -1.0270e-02,  4.0974e-02],\n",
            "        [ 3.0313e-02,  5.7074e-02, -3.0086e-02, -4.2521e-02,  1.0845e-01,\n",
            "          7.1810e-02, -5.3368e-02, -2.2635e-02, -9.4917e-02, -9.2737e-02,\n",
            "         -1.0154e-02, -2.3036e-02, -1.9389e-02,  2.0157e-02,  5.6758e-03,\n",
            "          6.7923e-02, -3.5974e-02,  6.5580e-03, -3.5154e-02,  8.3983e-02,\n",
            "          8.4637e-02,  6.3489e-02, -6.6571e-02, -6.4665e-02,  1.0325e-01,\n",
            "         -3.5084e-02,  4.3990e-02, -5.1299e-02, -3.2559e-02, -2.8319e-02,\n",
            "          4.4890e-03,  1.4636e-02, -7.0832e-02,  1.8635e-02, -7.3360e-02,\n",
            "          1.0078e-02,  7.4167e-02, -1.0854e-01, -1.6610e-02, -2.7690e-03,\n",
            "          3.2934e-02, -6.5267e-03, -2.5634e-02, -6.0582e-02,  2.5228e-02,\n",
            "         -2.6346e-02, -2.2758e-02,  7.3987e-02,  8.8332e-02, -1.0351e-01,\n",
            "          3.7687e-03,  3.9730e-02, -9.8720e-03, -8.2017e-02, -6.0294e-02,\n",
            "         -1.1172e-02, -9.0659e-02,  3.6549e-02,  1.0758e-01, -6.9538e-02,\n",
            "          7.6955e-02,  9.0696e-02, -3.4700e-02,  3.2562e-03, -1.9177e-02,\n",
            "         -9.2361e-02,  1.5329e-02,  9.8917e-02,  8.2161e-02, -3.0745e-02,\n",
            "          5.8168e-02,  2.5794e-02,  6.8467e-02,  9.6781e-02, -7.3800e-02,\n",
            "         -2.7723e-02, -7.2753e-02,  2.1635e-02, -4.2176e-04, -3.7836e-02,\n",
            "          1.9004e-02, -3.4061e-02,  6.3031e-02, -1.5683e-03],\n",
            "        [ 1.0285e-01,  1.0490e-02, -2.7106e-02, -1.0317e-01, -5.7729e-02,\n",
            "         -2.6239e-03, -8.5096e-02, -4.7830e-02,  8.6192e-02,  5.8539e-02,\n",
            "         -9.9870e-02,  2.5069e-02,  9.3292e-02, -6.7308e-02, -1.5615e-02,\n",
            "         -3.2976e-02,  4.2203e-02, -2.4397e-02, -4.2464e-02,  8.1278e-02,\n",
            "          9.5504e-02, -5.9997e-02, -6.1568e-02, -9.6189e-02,  3.0344e-02,\n",
            "         -9.5411e-02,  2.4146e-02, -4.4474e-02,  5.0287e-02, -2.3486e-02,\n",
            "         -3.9302e-02,  1.0486e-01, -4.6512e-02, -1.4645e-02, -5.4807e-03,\n",
            "          1.0485e-01,  5.0304e-03,  9.9725e-04, -7.2117e-02, -2.7774e-02,\n",
            "         -3.8630e-02,  1.0642e-01,  7.3266e-02,  4.7012e-02, -5.4467e-02,\n",
            "          6.8882e-02,  4.6170e-02, -8.4380e-02, -2.8603e-02, -6.8713e-02,\n",
            "         -6.9400e-02,  1.0310e-01, -4.5596e-02,  3.7214e-03, -2.6357e-03,\n",
            "         -6.6916e-03,  9.4718e-02, -9.0698e-02, -1.5462e-02,  5.4115e-02,\n",
            "          6.5915e-02, -7.3631e-02, -5.1977e-02, -3.6675e-02, -9.7981e-03,\n",
            "         -8.5741e-02,  2.0347e-02, -9.1450e-02,  2.5592e-02,  6.2070e-02,\n",
            "         -3.1904e-02,  2.2659e-02,  6.3577e-02, -4.0229e-02, -1.0223e-01,\n",
            "          1.2833e-04,  7.1063e-02,  5.9083e-02, -4.2142e-02, -6.8767e-02,\n",
            "          7.0364e-02, -3.7492e-02,  4.6754e-03, -9.2495e-02],\n",
            "        [ 3.7298e-02, -3.8221e-02,  5.1562e-02, -7.1721e-02, -6.1558e-02,\n",
            "         -9.5623e-02, -7.0334e-02, -8.2848e-02,  3.9314e-02, -1.0534e-01,\n",
            "         -7.5026e-02, -5.4795e-02, -6.3995e-02,  4.4621e-02,  7.5748e-02,\n",
            "          9.2357e-02, -2.5859e-02,  4.8090e-02, -9.1733e-02, -4.7130e-02,\n",
            "          2.4889e-02,  2.2427e-02,  3.1588e-02,  3.6726e-03, -6.4960e-02,\n",
            "          9.2800e-02,  8.0086e-02,  4.1364e-02, -5.8960e-03, -6.3847e-03,\n",
            "         -3.5146e-02, -8.2579e-02,  6.4516e-02, -2.6771e-02,  2.2398e-02,\n",
            "          2.9880e-02,  2.3323e-03, -2.3962e-02, -1.0131e-01, -3.7288e-02,\n",
            "         -8.9862e-02,  4.9320e-02,  3.7280e-02,  6.5858e-02, -5.2745e-02,\n",
            "          6.8194e-02,  1.0328e-01, -1.5528e-02, -4.0931e-02, -1.8314e-05,\n",
            "         -1.0285e-01, -3.8762e-02,  8.1746e-02, -6.0936e-02,  3.3833e-02,\n",
            "         -9.5907e-02,  9.2670e-02,  8.9310e-02,  8.2830e-02, -3.9452e-02,\n",
            "         -1.0337e-01, -9.2764e-02, -3.8372e-02,  7.6684e-02,  2.3063e-02,\n",
            "          8.3113e-02, -5.5133e-02,  6.4692e-02,  9.9790e-02,  5.7144e-02,\n",
            "         -7.7759e-03, -3.6460e-02, -5.1920e-02,  3.2377e-02,  4.3818e-02,\n",
            "          3.1686e-02, -9.1981e-02, -7.5148e-02,  3.9272e-02, -9.7155e-03,\n",
            "         -4.2300e-02, -2.5311e-02, -6.5875e-02, -5.3785e-02]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([-0.0663,  0.0308, -0.0664,  0.0726,  0.1060,  0.0526, -0.0149,  0.0093,\n",
            "         0.0105, -0.0491], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8avbmih7MOW",
        "colab_type": "text"
      },
      "source": [
        "Let try a random 32x32 input\n",
        "Note: Expected input size to this net(LeNet) is 32x32. To use this net on\n",
        "MNIST dataset, please resize the images from the dataset to 32x32.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxjZOW1P7MOX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "03965818-9836-40bb-ee37-d02e9a424a2b"
      },
      "source": [
        "input = torch.randn(1, 1, 32, 32)\n",
        "out = net(input)\n",
        "print(out)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0670,  0.0805, -0.0960,  0.0579,  0.0820,  0.0284,  0.0384,  0.0020,\n",
            "         -0.0106, -0.0712]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8_noezw7MOa",
        "colab_type": "text"
      },
      "source": [
        "Zero the gradient buffers of all parameters and backprops with random\n",
        "gradients:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1EGHPdm7MOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net.zero_grad()\n",
        "out.backward(torch.randn(1, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJFNQFTD7MOe",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.nn`` only supports mini-batches. The entire ``torch.nn``\n",
        "    package only supports inputs that are a mini-batch of samples, and not\n",
        "    a single sample.\n",
        "\n",
        "    For example, ``nn.Conv2d`` will take in a 4D Tensor of\n",
        "    ``nSamples x nChannels x Height x Width``.\n",
        "\n",
        "    If you have a single sample, just use ``input.unsqueeze(0)`` to add\n",
        "    a fake batch dimension.</p></div>\n",
        "\n",
        "Before proceeding further, let's recap all the classes you’ve seen so far.\n",
        "\n",
        "**Recap:**\n",
        "  -  ``torch.Tensor`` - A *multi-dimensional array* with support for autograd\n",
        "     operations like ``backward()``. Also *holds the gradient* w.r.t. the\n",
        "     tensor.\n",
        "  -  ``nn.Module`` - Neural network module. *Convenient way of\n",
        "     encapsulating parameters*, with helpers for moving them to GPU,\n",
        "     exporting, loading, etc.\n",
        "  -  ``nn.Parameter`` - A kind of Tensor, that is *automatically\n",
        "     registered as a parameter when assigned as an attribute to a*\n",
        "     ``Module``.\n",
        "  -  ``autograd.Function`` - Implements *forward and backward definitions\n",
        "     of an autograd operation*. Every ``Tensor`` operation, creates at\n",
        "     least a single ``Function`` node, that connects to functions that\n",
        "     created a ``Tensor`` and *encodes its history*.\n",
        "\n",
        "**At this point, we covered:**\n",
        "  -  Defining a neural network\n",
        "  -  Processing inputs and calling backward\n",
        "\n",
        "**Still Left:**\n",
        "  -  Computing the loss\n",
        "  -  Updating the weights of the network\n",
        "\n",
        "Loss Function\n",
        "-------------\n",
        "A loss function takes the (output, target) pair of inputs, and computes a\n",
        "value that estimates how far away the output is from the target.\n",
        "\n",
        "There are several different\n",
        "`loss functions <http://pytorch.org/docs/nn.html#loss-functions>`_ under the\n",
        "nn package .\n",
        "A simple loss is: ``nn.MSELoss`` which computes the mean-squared error\n",
        "between the input and the target.\n",
        "\n",
        "For example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY5g8plH7MOf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ecae14c5-cd51-438d-da89-d19c0ed961ed"
      },
      "source": [
        "output = net(input)\n",
        "target = torch.randn(10)  # a dummy target, for example\n",
        "target = target.view(1, -1)  # make it the same shape as output\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.4681, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o13I4V3s7MOi",
        "colab_type": "text"
      },
      "source": [
        "Now, if you follow ``loss`` in the backward direction, using its\n",
        "``.grad_fn`` attribute, you will see a graph of computations that looks\n",
        "like this:\n",
        "\n",
        "::\n",
        "\n",
        "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
        "          -> view -> linear -> relu -> linear -> relu -> linear\n",
        "          -> MSELoss\n",
        "          -> loss\n",
        "\n",
        "So, when we call ``loss.backward()``, the whole graph is differentiated\n",
        "w.r.t. the loss, and all Tensors in the graph that has ``requires_grad=True``\n",
        "will have their ``.grad`` Tensor accumulated with the gradient.\n",
        "\n",
        "For illustration, let us follow a few steps backward:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUbUH_GW7MOj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4821530b-5f0e-4d38-ebf9-47035f60749e"
      },
      "source": [
        "print(loss.grad_fn)  # MSELoss\n",
        "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MseLossBackward object at 0x7fa00c247dd8>\n",
            "<AddmmBackward object at 0x7fa00c247dd8>\n",
            "<AccumulateGrad object at 0x7fa05864feb8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btFG3zUv7MOm",
        "colab_type": "text"
      },
      "source": [
        "Backprop\n",
        "--------\n",
        "To backpropagate the error all we have to do is to ``loss.backward()``.\n",
        "You need to clear the existing gradients though, else gradients will be\n",
        "accumulated to existing gradients.\n",
        "\n",
        "\n",
        "Now we shall call ``loss.backward()``, and have a look at conv1's bias\n",
        "gradients before and after the backward.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ewwtey77MOn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d92386b8-3eda-4fe6-8659-bfa97148d9d5"
      },
      "source": [
        "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1.bias.grad before backward\n",
            "tensor([0., 0., 0., 0., 0., 0.])\n",
            "conv1.bias.grad after backward\n",
            "tensor([ 0.0063,  0.0085,  0.0065, -0.0037, -0.0067,  0.0030])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xDBFULB7MOq",
        "colab_type": "text"
      },
      "source": [
        "Now, we have seen how to use loss functions.\n",
        "\n",
        "**Read Later:**\n",
        "\n",
        "  The neural network package contains various modules and loss functions\n",
        "  that form the building blocks of deep neural networks. A full list with\n",
        "  documentation is `here <http://pytorch.org/docs/nn>`_.\n",
        "\n",
        "**The only thing left to learn is:**\n",
        "\n",
        "  - Updating the weights of the network\n",
        "\n",
        "Update the weights\n",
        "------------------\n",
        "The simplest update rule used in practice is the Stochastic Gradient\n",
        "Descent (SGD):\n",
        "\n",
        "     ``weight = weight - learning_rate * gradient``\n",
        "\n",
        "We can implement this using simple python code:\n",
        "\n",
        ".. code:: python\n",
        "\n",
        "    learning_rate = 0.01\n",
        "    for f in net.parameters():\n",
        "        f.data.sub_(f.grad.data * learning_rate)\n",
        "\n",
        "However, as you use neural networks, you want to use various different\n",
        "update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n",
        "To enable this, we built a small package: ``torch.optim`` that\n",
        "implements all these methods. Using it is very simple:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkaPzSOJ7MOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# in your training loop:\n",
        "optimizer.zero_grad()   # zero the gradient buffers\n",
        "output = net(input)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step()    # Does the update"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17185Rvf7MOu",
        "colab_type": "text"
      },
      "source": [
        ".. Note::\n",
        "\n",
        "      Observe how gradient buffers had to be manually set to zero using\n",
        "      ``optimizer.zero_grad()``. This is because gradients are accumulated\n",
        "      as explained in `Backprop`_ section.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_ckK1dABB6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}